{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c044a9f",
      "metadata": {
        "id": "7c044a9f",
        "outputId": "413f4824-310e-4583-9f27-66285dc378fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Collecting mlflow-skinny==3.1.4 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading databricks_sdk-0.59.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.116.1)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (4.14.1)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.47.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.23.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (2025.7.14)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.4->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.1.4-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.4-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.59.0-py3-none-any.whl (676 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.2/676.2 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=1a6a1c7f701fbd66b6488d7cfff0cb71220ca25f4607797cc4b6263dd0e180c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: gunicorn, graphql-core, opentelemetry-api, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, lime, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed alembic-1.16.4 databricks-sdk-0.59.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 lime-0.2.0.1 mlflow-3.1.4 mlflow-skinny-3.1.4 opentelemetry-api-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0\n"
          ]
        }
      ],
      "source": [
        "pip install lime joblib mlflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "ILKjMalEtYXZ",
        "outputId": "1211fe5d-38ec-4d69-b68d-40c177e60936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ILKjMalEtYXZ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a825d78c",
      "metadata": {
        "id": "a825d78c"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d239ec9d",
      "metadata": {
        "id": "d239ec9d"
      },
      "source": [
        "# Load Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4498f308",
      "metadata": {
        "id": "4498f308"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy import sparse\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('models/fraud_data', exist_ok=True)\n",
        "os.makedirs('models/credit_card_data', exist_ok=True)\n",
        "logger.info(\"Model directories created.\")\n",
        "\n",
        "# Load data\n",
        "fraud_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/processed/cleaned_fraud_data_by_country.csv')\n",
        "credit_card_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/processed/cleaned_credit_card_data.csv')\n",
        "\n",
        "\n",
        "# Datetime features for fraud data\n",
        "fraud_data['signup_time'] = pd.to_datetime(fraud_data['signup_time'])\n",
        "fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])\n",
        "\n",
        "fraud_data['signup_hour'] = fraud_data['signup_time'].dt.hour\n",
        "fraud_data['signup_day'] = fraud_data['signup_time'].dt.day\n",
        "fraud_data['signup_month'] = fraud_data['signup_time'].dt.month\n",
        "fraud_data['signup_year'] = fraud_data['signup_time'].dt.year\n",
        "\n",
        "fraud_data['purchase_hour'] = fraud_data['purchase_time'].dt.hour\n",
        "fraud_data['purchase_day'] = fraud_data['purchase_time'].dt.day\n",
        "fraud_data['purchase_month'] = fraud_data['purchase_time'].dt.month\n",
        "fraud_data['purchase_year'] = fraud_data['purchase_time'].dt.year\n",
        "\n",
        "fraud_data = fraud_data.drop(columns=['signup_time', 'purchase_time'])\n",
        "\n",
        "# Frequency encoding for categorical columns\n",
        "cat_cols = fraud_data.select_dtypes(include='object').columns\n",
        "for col in cat_cols:\n",
        "    freq_map = fraud_data[col].value_counts().to_dict()\n",
        "    fraud_data[col] = fraud_data[col].map(freq_map)\n",
        "\n",
        "# Downcast numeric types\n",
        "for col in fraud_data.select_dtypes(include='int64'):\n",
        "    fraud_data[col] = fraud_data[col].astype('int32')\n",
        "for col in fraud_data.select_dtypes(include='float64'):\n",
        "    fraud_data[col] = fraud_data[col].astype('float32')\n",
        "\n",
        "# Separate features and targets\n",
        "X_fraud = fraud_data.drop(columns=['class'])\n",
        "y_fraud = fraud_data['class']\n",
        "\n",
        "X_credit = credit_card_data.drop(columns=['Class'])\n",
        "y_credit = credit_card_data['Class']\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_fraud = imputer.fit_transform(X_fraud)\n",
        "X_credit = imputer.fit_transform(X_credit)\n",
        "\n",
        "y_fraud = y_fraud.fillna(y_fraud.mode()[0])\n",
        "y_credit = y_credit.fillna(y_credit.mode()[0])\n",
        "\n",
        "# Convert to sparse for fraud (optional)\n",
        "X_fraud_sparse = sparse.csr_matrix(X_fraud)\n",
        "\n",
        "# Train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(X_fraud_sparse, y_fraud, test_size=0.2, random_state=42)\n",
        "X_train_credit, X_test_credit, y_train_credit, y_test_credit = train_test_split(X_credit, y_credit, test_size=0.2, random_state=42)\n",
        "\n",
        "logger.info(\"Data loading and preprocessing complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf0d3ef",
      "metadata": {
        "id": "1cf0d3ef"
      },
      "source": [
        "# I Have Selected 3 Model for Comparing Performance\n",
        "1) Decison Tree\n",
        "2) logistic Regression\n",
        "3) Random Forest !!\n",
        "\n",
        "### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8852b0",
      "metadata": {
        "id": "8c8852b0",
        "outputId": "ce949edb-339b-4f85-ae48-18129fd29eaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-21 21:12:35,928 - DEBUG - Popen(['git', 'version'], cwd=c:\\Users\\HP\\10 Acadamy PRojects\\New folder (8 & 9)\\Ehanced_Fraud_Detection_Ecommerce_Banking\\notebooks, stdin=None, shell=False, universal_newlines=False)\n",
            "2025-07-21 21:12:36,262 - DEBUG - Popen(['git', 'version'], cwd=c:\\Users\\HP\\10 Acadamy PRojects\\New folder (8 & 9)\\Ehanced_Fraud_Detection_Ecommerce_Banking\\notebooks, stdin=None, shell=False, universal_newlines=False)\n",
            "2025-07-21 21:12:36,480 - DEBUG - Popen(['git', 'check-ignore', 'c:\\\\Users\\\\HP\\\\10 Acadamy PRojects\\\\New folder (8 & 9)\\\\Ehanced_Fraud_Detection_Ecommerce_Banking\\\\.venv\\\\Lib\\\\site-packages'], cwd=c:\\Users\\HP\\10 Acadamy PRojects\\New folder (8 & 9)\\Ehanced_Fraud_Detection_Ecommerce_Banking, stdin=None, shell=False, universal_newlines=False)\n",
            "2025-07-21 21:12:37,107 - INFO - Training Logistic Regression on Fraud Data\n",
            "2025/07/21 21:12:42 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/21 21:13:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025-07-21 21:13:15,899 - INFO - Saved Logistic Regression to models/fraud_data/logistic_regression.joblib\n",
            "2025-07-21 21:13:16,102 - INFO - Training Decision Tree on Fraud Data\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Model: Logistic Regression | Dataset: Fraud Data\n",
            "✅ Accuracy: 0.6478\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.64      0.77     27373\n",
            "           1       0.17      0.70      0.27      2850\n",
            "\n",
            "    accuracy                           0.65     30223\n",
            "   macro avg       0.56      0.67      0.52     30223\n",
            "weighted avg       0.88      0.65      0.72     30223\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/21 21:14:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/21 21:14:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025-07-21 21:14:42,803 - INFO - Saved Decision Tree to models/fraud_data/decision_tree.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Model: Decision Tree | Dataset: Fraud Data\n",
            "✅ Accuracy: 0.9211\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96     27373\n",
            "           1       0.58      0.59      0.59      2850\n",
            "\n",
            "    accuracy                           0.92     30223\n",
            "   macro avg       0.77      0.77      0.77     30223\n",
            "weighted avg       0.92      0.92      0.92     30223\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-21 21:14:43,088 - INFO - Training Random Forest on Fraud Data\n",
            "2025/07/21 21:28:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/21 21:29:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025-07-21 21:29:09,453 - INFO - Saved Random Forest to models/fraud_data/random_forest.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Model: Random Forest | Dataset: Fraud Data\n",
            "✅ Accuracy: 0.9565\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98     27373\n",
            "           1       1.00      0.54      0.70      2850\n",
            "\n",
            "    accuracy                           0.96     30223\n",
            "   macro avg       0.98      0.77      0.84     30223\n",
            "weighted avg       0.96      0.96      0.95     30223\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-21 21:29:09,743 - INFO - Training Logistic Regression on Credit Card Data\n",
            "c:\\Users\\HP\\10 Acadamy PRojects\\New folder (8 & 9)\\Ehanced_Fraud_Detection_Ecommerce_Banking\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "2025/07/21 21:30:41 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/21 21:31:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025-07-21 21:31:02,311 - INFO - Saved Logistic Regression to models/credit_card_data/logistic_regression.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Model: Logistic Regression | Dataset: Credit Card Data\n",
            "✅ Accuracy: 0.9767\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     56656\n",
            "           1       0.06      0.89      0.11        90\n",
            "\n",
            "    accuracy                           0.98     56746\n",
            "   macro avg       0.53      0.93      0.55     56746\n",
            "weighted avg       1.00      0.98      0.99     56746\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-21 21:31:02,886 - INFO - Training Decision Tree on Credit Card Data\n",
            "2025/07/21 21:31:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/21 21:32:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025-07-21 21:32:13,159 - INFO - Saved Decision Tree to models/credit_card_data/decision_tree.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Model: Decision Tree | Dataset: Credit Card Data\n",
            "✅ Accuracy: 0.9989\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56656\n",
            "           1       0.66      0.63      0.64        90\n",
            "\n",
            "    accuracy                           1.00     56746\n",
            "   macro avg       0.83      0.82      0.82     56746\n",
            "weighted avg       1.00      1.00      1.00     56746\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-21 21:32:13,451 - INFO - Training Random Forest on Credit Card Data\n",
            "2025/07/21 21:38:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/21 21:39:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025-07-21 21:39:02,219 - INFO - Saved Random Forest to models/credit_card_data/random_forest.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Model: Random Forest | Dataset: Credit Card Data\n",
            "✅ Accuracy: 0.9995\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56656\n",
            "           1       0.97      0.71      0.82        90\n",
            "\n",
            "    accuracy                           1.00     56746\n",
            "   macro avg       0.98      0.86      0.91     56746\n",
            "weighted avg       1.00      1.00      1.00     56746\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Setup MLflow\n",
        "mlflow.set_tracking_uri(\"file:///mlruns\")\n",
        "mlflow.set_experiment(\"Fraud and Credit Card Detection\")\n",
        "\n",
        "# Define only the required models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "}\n",
        "\n",
        "# Define the train and eval function\n",
        "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, dataset_name, model_name):\n",
        "    with mlflow.start_run(run_name=f'{model_name} on {dataset_name}'):\n",
        "        logger.info(f\"Training {model_name} on {dataset_name}\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred)\n",
        "\n",
        "        mlflow.log_params({\n",
        "            'model': model_name,\n",
        "            'dataset': dataset_name,\n",
        "            'test_size': 0.2,\n",
        "            'random_state': 42\n",
        "        })\n",
        "        mlflow.log_metric('accuracy', accuracy)\n",
        "        mlflow.sklearn.log_model(model, f'{model_name}_mlflow')\n",
        "\n",
        "        # Save locally\n",
        "        save_path = f\"models/{dataset_name.lower().replace(' ', '_')}/{model_name.lower().replace(' ', '_')}.joblib\"\n",
        "        joblib.dump(model, save_path)\n",
        "        logger.info(f\"Saved {model_name} to {save_path}\")\n",
        "\n",
        "        # Output results\n",
        "        print(f\"\\n📊 Model: {model_name} | Dataset: {dataset_name}\")\n",
        "        print(f\"✅ Accuracy: {accuracy:.4f}\")\n",
        "        print(report)\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "# Run on Fraud Data\n",
        "for name, model in models.items():\n",
        "    train_and_evaluate_model(model, X_train_fraud, y_train_fraud, X_test_fraud, y_test_fraud, 'Fraud Data', name)\n",
        "\n",
        "# Run on Credit Card Data\n",
        "for name, model in models.items():\n",
        "    train_and_evaluate_model(model, X_train_credit, y_train_credit, X_test_credit, y_test_credit, 'Credit Card Data', name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_iWlU2xu7Yh"
      },
      "id": "T_iWlU2xu7Yh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c622e62f",
      "metadata": {
        "id": "c622e62f"
      },
      "source": [
        "## Model Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5a67b8bb",
      "metadata": {
        "id": "5a67b8bb"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import lime\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import lime.lime_tabular\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "61236be7",
      "metadata": {
        "id": "61236be7",
        "outputId": "21e0c38c-3067-455a-cf1b-68acf9bee020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['user_id', 'purchase_value', 'device_id', 'age', 'ip_address', 'class',\n",
            "       'transaction_count', 'time_since_signup', 'transaction_velocity',\n",
            "       'transaction_time_diff', 'hour_of_day', 'day_of_week', 'source_Direct',\n",
            "       'source_SEO', 'browser_FireFox', 'browser_IE', 'browser_Opera',\n",
            "       'browser_Safari', 'sex_M', 'signup_hour', 'signup_day', 'signup_month',\n",
            "       'signup_year', 'purchase_hour', 'purchase_day', 'purchase_month',\n",
            "       'purchase_year'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(fraud_data.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "25301b5a",
      "metadata": {
        "id": "25301b5a"
      },
      "outputs": [],
      "source": [
        "# Reconstruct datetime from parts in fraud_data\n",
        "fraud_data['signup_datetime'] = pd.to_datetime(dict(\n",
        "    year=fraud_data['signup_year'],\n",
        "    month=fraud_data['signup_month'],\n",
        "    day=fraud_data['signup_day'],\n",
        "    hour=fraud_data['signup_hour']\n",
        "))\n",
        "\n",
        "fraud_data['purchase_datetime'] = pd.to_datetime(dict(\n",
        "    year=fraud_data['purchase_year'],\n",
        "    month=fraud_data['purchase_month'],\n",
        "    day=fraud_data['purchase_day'],\n",
        "    hour=fraud_data['purchase_hour']\n",
        "))\n",
        "\n",
        "# Optional: convert these to UNIX timestamps\n",
        "fraud_data['signup_timestamp'] = fraud_data['signup_datetime'].astype('int64') / 10**9\n",
        "fraud_data['purchase_timestamp'] = fraud_data['purchase_datetime'].astype('int64') / 10**9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a4f20acf",
      "metadata": {
        "id": "a4f20acf"
      },
      "outputs": [],
      "source": [
        " # Prepare features and target\n",
        "X_fraud = fraud_data.drop(columns=['class'])\n",
        "y_fraud = fraud_data['class']\n",
        "\n",
        "# Keep only numeric columns (exclude things like user_id, device_id, etc.)\n",
        "X_fraud = X_fraud.select_dtypes(include='number')\n",
        "\n",
        "# Repeat for credit card dataset\n",
        "X_credit = credit_card_data.drop(columns=['Class'])\n",
        "y_credit = credit_card_data['Class']\n",
        "X_credit = X_credit.select_dtypes(include='number')\n",
        "\n",
        "# Split into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
        "    X_fraud, y_fraud, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train_credit, X_test_credit, y_train_credit, y_test_credit = train_test_split(\n",
        "    X_credit, y_credit, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9da90177",
      "metadata": {
        "id": "9da90177"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "fraud_model = joblib.load('/content/drive/MyDrive/Colab Notebooks/models/fraud_data/random_forest.joblib')\n",
        "credit_model = joblib.load('/content/drive/MyDrive/Colab Notebooks/models/credit_card_data/random_forest.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP for Fraud Detection Model"
      ],
      "metadata": {
        "id": "-69mwWzWz0rX"
      },
      "id": "-69mwWzWz0rX"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fa1e0b0d",
      "metadata": {
        "id": "fa1e0b0d"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Enable JS visualizations (optional for notebooks)\n",
        "shap.initjs()\n",
        "\n",
        "# Sample 100 rows for efficiency\n",
        "X_sample = X_test_fraud.sample(n=100, random_state=42)\n",
        "\n",
        "# Use TreeExplainer on your best-performing model (RandomForest, XGBoost, etc.)\n",
        "explainer = shap.TreeExplainer(fraud_model)\n",
        "\n",
        "# Compute SHAP values (for binary classification → use index [1])\n",
        "shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "# Global Feature Importance (Summary Plot)\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values[1], X_sample)\n",
        "plt.show()\n",
        "\n",
        "# Local Explanation for a Single Prediction (first row in sample)\n",
        "plt.figure()\n",
        "shap.force_plot(\n",
        "    explainer.expected_value[1],\n",
        "    shap_values[1][0],\n",
        "    X_sample.iloc[0],\n",
        "    matplotlib=True\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# Dependence Plot (replace with a real feature name)\n",
        "# Find the most important feature from the summary plot and use it here\n",
        "top_feature = X_sample.columns[0]  # Or replace manually with 'signup_timestamp' or similar\n",
        "plt.figure()\n",
        "shap.dependence_plot(top_feature, shap_values[1], X_sample)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP for Credit Card Model"
      ],
      "metadata": {
        "id": "K6mpFKK7z6uU"
      },
      "id": "K6mpFKK7z6uU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfae311a",
      "metadata": {
        "id": "dfae311a"
      },
      "outputs": [],
      "source": [
        "explainer_credit = shap.TreeExplainer(credit_model)\n",
        "shap_values_credit = explainer_credit.shap_values(X_test_credit)\n",
        "\n",
        "shap.summary_plot(shap_values_credit[1], X_test_credit)\n",
        "plt.show()\n",
        "\n",
        "shap.force_plot(\n",
        "    explainer_credit.expected_value[1],\n",
        "    shap_values_credit[1][0],\n",
        "    X_test_credit.iloc[0],\n",
        "    matplotlib=True\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "shap.dependence_plot('Amount', shap_values_credit[1], X_test_credit)  # or another numeric feature\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LIME for Fraud Model"
      ],
      "metadata": {
        "id": "GtOJkiLA0CpS"
      },
      "id": "GtOJkiLA0CpS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2007a28e",
      "metadata": {
        "id": "2007a28e"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "explainer_lime_fraud = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train_fraud.values,\n",
        "    feature_names=X_train_fraud.columns.tolist(),\n",
        "    class_names=['Not Fraud', 'Fraud'],\n",
        "    mode='classification',\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "explanation_fraud = explainer_lime_fraud.explain_instance(\n",
        "    X_test_fraud.iloc[0].values,\n",
        "    fraud_model.predict_proba,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "explanation_fraud.show_in_notebook(show_all=False)\n",
        "explanation_fraud.as_pyplot_figure()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LIME for Credit Card Model"
      ],
      "metadata": {
        "id": "F-kSNsDT0HZp"
      },
      "id": "F-kSNsDT0HZp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb97c568",
      "metadata": {
        "id": "fb97c568"
      },
      "outputs": [],
      "source": [
        "explainer_lime_credit = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train_credit.values,\n",
        "    feature_names=X_train_credit.columns.tolist(),\n",
        "    class_names=['Class 0', 'Class 1'],\n",
        "    mode='classification',\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "explanation_credit = explainer_lime_credit.explain_instance(\n",
        "    X_test_credit.iloc[0].values,\n",
        "    credit_model.predict_proba,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "explanation_credit.show_in_notebook(show_all=False)\n",
        "explanation_credit.as_pyplot_figure()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14662f8e",
      "metadata": {
        "id": "14662f8e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f1f068",
      "metadata": {
        "id": "f0f1f068"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}